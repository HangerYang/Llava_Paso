{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import sys\n",
    "sys.path.append('/home/hyang/llava_paso/LLaVA')\n",
    "\n",
    "from llava.mm_utils import get_model_name_from_path\n",
    "from llava.constants import (\n",
    "    IMAGE_TOKEN_INDEX,\n",
    "    DEFAULT_IMAGE_TOKEN,\n",
    "    DEFAULT_IM_START_TOKEN,\n",
    "    DEFAULT_IM_END_TOKEN,\n",
    ")\n",
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.utils import disable_torch_init\n",
    "from llava.mm_utils import (\n",
    "    tokenizer_image_token,\n",
    "    get_model_name_from_path,\n",
    "    KeywordsStoppingCriteria,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hades(mm_use_im_start_end=False, model_name=\"vicuna-7b-v1.5\"):\n",
    "    hades = load_dataset(\"Monosail/HADES\")['test']\n",
    "    hades = [item for item in hades if item['step'] == 5]\n",
    "    malicious = []\n",
    "    for item in hades:\n",
    "        image = item['image'].convert('RGB')\n",
    "        inst = item['instruction']\n",
    "\n",
    "        # if item[\"category\"] == \"behavior\":\n",
    "        #     mask = \"conduct this behavior on\"\n",
    "        # else:\n",
    "        #     mask = f\"this {item['category']}\"\n",
    "        # new_inst = f\"The bottom of the image shows a {item['category']}. \" + inst.replace(item[\"keywords\"], mask)\n",
    "        qs = inst\n",
    "        if mm_use_im_start_end:\n",
    "            qs = (\n",
    "                DEFAULT_IM_START_TOKEN\n",
    "                + DEFAULT_IMAGE_TOKEN\n",
    "                + DEFAULT_IM_END_TOKEN\n",
    "                + \"\\n\"\n",
    "                + qs\n",
    "            )\n",
    "        else:\n",
    "            qs = DEFAULT_IMAGE_TOKEN + \"\\n\" + qs\n",
    "\n",
    "        if \"llama-2\" in model_name.lower():\n",
    "            conv_mode = \"llava_llama_2\"\n",
    "        elif \"v1\" in model_name.lower():\n",
    "            conv_mode = \"llava_v1\"\n",
    "        elif \"mpt\" in model_name.lower():\n",
    "            conv_mode = \"mpt\"\n",
    "        else:\n",
    "            conv_mode = \"llava_v0\"\n",
    "\n",
    "        conv = conv_templates[conv_mode].copy()\n",
    "        conv.append_message(conv.roles[0], qs)\n",
    "        conv.append_message(conv.roles[1], None)\n",
    "        prompt = conv.get_prompt()\n",
    "        malicious.append({'id':item['id'], 'image':image,'prompt': prompt})\n",
    "    return malicious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "750"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(load_hades())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_normal(path, mm_use_im_start_end=False,model_name=\"vicuna-7b-v1.5\"):\n",
    "    with open(path, \"r\") as file:\n",
    "        content = file.read()\n",
    "    normal_dataset = json.loads(content)\n",
    "    normal = []\n",
    "    for item in tqdm(normal_dataset[:750]):\n",
    "        image = Image.open(\"/home/hyang/llava_paso/coco/\" + item['image']).convert('RGB').resize((1024, 1324))\n",
    "        for conv_item in item['conversations']:\n",
    "            if conv_item['from'] == 'human':\n",
    "                qs = conv_item['value']\n",
    "                if mm_use_im_start_end:\n",
    "                    qs = (\n",
    "                        DEFAULT_IM_START_TOKEN\n",
    "                        + DEFAULT_IMAGE_TOKEN\n",
    "                        + DEFAULT_IM_END_TOKEN\n",
    "                        + \"\\n\"\n",
    "                        + qs\n",
    "                    )\n",
    "                else:\n",
    "                    qs = DEFAULT_IMAGE_TOKEN + \"\\n\" + qs\n",
    "\n",
    "                if \"llama-2\" in model_name.lower():\n",
    "                    conv_mode = \"llava_llama_2\"\n",
    "                elif \"v1\" in model_name.lower():\n",
    "                    conv_mode = \"llava_v1\"\n",
    "                elif \"mpt\" in model_name.lower():\n",
    "                    conv_mode = \"mpt\"\n",
    "                else:\n",
    "                    conv_mode = \"llava_v0\"\n",
    "\n",
    "                conv = conv_templates[conv_mode].copy()\n",
    "                conv.append_message(conv.roles[0], qs)\n",
    "                conv.append_message(conv.roles[1], None)\n",
    "                prompt = conv.get_prompt()\n",
    "                normal.append({'id':item['id'], 'image':image,'prompt': prompt})\n",
    "    return normal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:12<00:00, 59.15it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': '000000033471',\n",
       " 'image': <PIL.Image.Image image mode=RGB size=1024x1324>,\n",
       " 'prompt': \"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <image>\\nWhat are the colors of the bus in the image?\\n<image> ASSISTANT:\"}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_normal(path=\"/home/hyang/llava_paso/coco/conversation_58k.json\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "        model_path=\"liuhaotian/llava-v1.5-7b\", model_base=None, model_name=\"llava-v1.5-7b\",cache_dir = \"/home/hyang/llava_paso/.cache\"\n",
    "    )\n",
    "model_name = get_model_name_from_path(\"liuhaotian/llava-v1.5-7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs = new_inst\n",
    "if model.config.mm_use_im_start_end:\n",
    "    qs = (\n",
    "        DEFAULT_IM_START_TOKEN\n",
    "        + DEFAULT_IMAGE_TOKEN\n",
    "        + DEFAULT_IM_END_TOKEN\n",
    "        + \"\\n\"\n",
    "        + qs\n",
    "    )\n",
    "else:\n",
    "    qs = DEFAULT_IMAGE_TOKEN + \"\\n\" + qs\n",
    "if \"llama-2\" in model_name.lower():\n",
    "        conv_mode = \"llava_llama_2\"\n",
    "elif \"v1\" in model_name.lower():\n",
    "    conv_mode = \"llava_v1\"\n",
    "elif \"mpt\" in model_name.lower():\n",
    "    conv_mode = \"mpt\"\n",
    "else:\n",
    "    conv_mode = \"llava_v0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = conv_templates[conv_mode].copy()\n",
    "conv.append_message(conv.roles[0], qs)\n",
    "conv.append_message(conv.roles[1], None)\n",
    "prompt = conv.get_prompt()\n",
    "\n",
    "image_tensor = (\n",
    "    image_processor.preprocess(image, return_tensors=\"pt\")[\"pixel_values\"]\n",
    "    .half()\n",
    "    .cuda()\n",
    ")\n",
    "\n",
    "input_ids = (\n",
    "    tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\")\n",
    "    .unsqueeze(0)\n",
    "    .cuda()\n",
    ")\n",
    "\n",
    "stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n",
    "keywords = [stop_str]\n",
    "stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            images=image_tensor,  # Pass image tensor to the model\n",
    "            output_hidden_states=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl_pair = []\n",
    "lm_head = model.lm_head\n",
    "decoding=True\n",
    "k_indices=5\n",
    "# Get normalization layer\n",
    "if hasattr(model, \"model\") and hasattr(model.model, \"norm\"):\n",
    "    norm = model.model.norm\n",
    "elif hasattr(model, \"transformer\") and hasattr(model.transformer, \"ln_f\"):\n",
    "    norm = model.transformer.ln_f\n",
    "else:\n",
    "    raise ValueError(\"Incorrect Model: Missing norm layer.\")\n",
    "\n",
    "# Process hidden states layer by layer\n",
    "for i, hidden_state in enumerate(outputs.hidden_states):\n",
    "    layer_logits = []\n",
    "    layer_output = norm(hidden_state)\n",
    "    logits = lm_head(layer_output)\n",
    "\n",
    "    # Get logits for the next token\n",
    "    next_token_logits = logits[:, -1, :]\n",
    "    top_values, top_indices = torch.topk(next_token_logits, k_indices, dim=-1)\n",
    "\n",
    "    # Decode or return raw token indices\n",
    "    decoded_texts = [tokenizer.decode([idx], skip_special_tokens=False) for idx in top_indices.squeeze().tolist()]\n",
    "    top_values = top_values.detach().cpu()\n",
    "    if decoding:\n",
    "        for value, token in zip(top_values.squeeze().tolist(), decoded_texts):\n",
    "            layer_logits.append([token, value])\n",
    "    else:\n",
    "        for value, token in zip(top_values.squeeze().tolist(), top_indices.squeeze().tolist()):\n",
    "            layer_logits.append([token, value])\n",
    "    tl_pair.append(layer_logits)\n",
    "\n",
    "# Extract hidden states for analysis\n",
    "res_hidden_states = [hidden_state.detach().cpu().numpy() for hidden_state in outputs.hidden_states]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Weak2StrongExplanation_VLM:\n",
    "    def __init__(self, model, tokenizer, layer_nums=32, return_report=True, return_visual=True):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.layer_sums = layer_nums + 1\n",
    "        self.forward_info = {}\n",
    "        self.return_report = return_report\n",
    "        self.return_visual = return_visual"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
