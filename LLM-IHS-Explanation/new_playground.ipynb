{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyang/anaconda3/envs/llava/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-12-19 18:38:36,196] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import sys\n",
    "import numpy as np\n",
    "import os\n",
    "from w2s_utils import get_layer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import accelerate\n",
    "from visualization import topk_intermediate_confidence_heatmap, accuracy_line\n",
    "import accelerate\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "sys.path.append('/home/hyang/llava_paso/LLaVA')\n",
    "\n",
    "from llava.mm_utils import get_model_name_from_path\n",
    "from llava.constants import (\n",
    "    IMAGE_TOKEN_INDEX,\n",
    "    DEFAULT_IMAGE_TOKEN,\n",
    "    DEFAULT_IM_START_TOKEN,\n",
    "    DEFAULT_IM_END_TOKEN,\n",
    ")\n",
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "from llava.model.builder import load_pretrained_model, load_pretrained_model_paper\n",
    "from llava.utils import disable_torch_init\n",
    "from llava.mm_utils import (\n",
    "    tokenizer_image_token,\n",
    "    get_model_name_from_path,\n",
    "    KeywordsStoppingCriteria,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hades(mm_use_im_start_end=False, model_name=\"vicuna-7b-v1.5\"):\n",
    "    hades = load_dataset(\"Monosail/HADES\")['test']\n",
    "    hades = [item for item in hades if item['step'] == 5]\n",
    "    malicious = []\n",
    "    for item in hades:\n",
    "        image = item['image'].convert('RGB')\n",
    "        # image = Image.open(\"/home/hyang/llava_paso/blank_white_image.png\")\n",
    "        inst = item['instruction']\n",
    "\n",
    "        # if item[\"category\"] == \"behavior\":\n",
    "        #     mask = \"conduct this behavior on\"\n",
    "        # else:\n",
    "        #     mask = f\"this {item['category']}\"\n",
    "        # new_inst = f\"The bottom of the image shows a {item['category']}. \" + inst.replace(item[\"keywords\"], mask)\n",
    "        qs = inst\n",
    "        qs = \" \"\n",
    "        if mm_use_im_start_end:\n",
    "            qs = (\n",
    "                DEFAULT_IM_START_TOKEN\n",
    "                + DEFAULT_IMAGE_TOKEN\n",
    "                + DEFAULT_IM_END_TOKEN\n",
    "                + \"\\n\"\n",
    "                + qs\n",
    "            )\n",
    "        else:\n",
    "            qs = DEFAULT_IMAGE_TOKEN + \"\\n\" + qs\n",
    "\n",
    "        if \"llama-2\" in model_name.lower():\n",
    "            conv_mode = \"llava_llama_2\"\n",
    "        elif \"v1\" in model_name.lower():\n",
    "            conv_mode = \"llava_v1\"\n",
    "        elif \"mpt\" in model_name.lower():\n",
    "            conv_mode = \"mpt\"\n",
    "        else:\n",
    "            conv_mode = \"llava_v0\"\n",
    "\n",
    "        conv = conv_templates[conv_mode].copy()\n",
    "        conv.append_message(conv.roles[0], qs)\n",
    "        conv.append_message(conv.roles[1], None)\n",
    "        prompt = conv.get_prompt()\n",
    "        malicious.append({'id':item['id'], 'image':image,'prompt': prompt})\n",
    "    return malicious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_normal(dataset_path, mm_use_im_start_end=False,model_name=\"vicuna-7b-v1.5\"):\n",
    "    with open(dataset_path, \"r\") as file:\n",
    "        content = file.read()\n",
    "    normal_dataset = json.loads(content)\n",
    "    normal = []\n",
    "    for item in tqdm(normal_dataset[:750]):\n",
    "        image = Image.open(\"/home/hyang/llava_paso/coco/\" + item['image']).convert('RGB').resize((1024, 1324))\n",
    "        # image = Image.open(\"/home/hyang/llava_paso/blank_white_image.png\")\n",
    "        for conv_item in item['conversations']:\n",
    "            if conv_item['from'] == 'human':\n",
    "                # qs = conv_item['value']\n",
    "                qs = \" \"\n",
    "                if mm_use_im_start_end:\n",
    "                    qs = (\n",
    "                        DEFAULT_IM_START_TOKEN\n",
    "                        + DEFAULT_IMAGE_TOKEN\n",
    "                        + DEFAULT_IM_END_TOKEN\n",
    "                        + \"\\n\"\n",
    "                        + qs\n",
    "                    )\n",
    "                elif DEFAULT_IMAGE_TOKEN not in qs:\n",
    "                    qs = DEFAULT_IMAGE_TOKEN + \"\\n\" + qs\n",
    "\n",
    "\n",
    "                if \"llama-2\" in model_name.lower():\n",
    "                    conv_mode = \"llava_llama_2\"\n",
    "                elif \"v1\" in model_name.lower():\n",
    "                    conv_mode = \"llava_v1\"\n",
    "                elif \"mpt\" in model_name.lower():\n",
    "                    conv_mode = \"mpt\"\n",
    "                else:\n",
    "                    conv_mode = \"llava_v0\"\n",
    "\n",
    "                conv = conv_templates[conv_mode].copy()\n",
    "                conv.append_message(conv.roles[0], qs)\n",
    "                conv.append_message(conv.roles[1], None)\n",
    "                prompt = conv.get_prompt()\n",
    "                normal.append({'id':item['id'], 'image':image,'prompt': prompt})\n",
    "    return normal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"lmsys/vicuna-7b-v1.5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyang/anaconda3/envs/llava/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/hyang/anaconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.69s/it]\n",
      "/home/hyang/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/hyang/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_path, output_hidden_states=True, device_map=\"auto\", cache_dir = \"/home/hyang/llava_paso/.cache\")\n",
    "model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, cache_dir = \"/home/hyang/llava_paso/.cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "harmful_data = load_hades()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = harmful_data[0]['prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_mode = \"llava_v1\"\n",
    "conv = conv_templates[conv_mode].copy()\n",
    "conv.append_message(conv.roles[0], query)\n",
    "conv.append_message(conv.roles[1], None)\n",
    "prompt = conv.get_prompt()\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"]\n",
    "input_ids = input_ids.to(model.device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
